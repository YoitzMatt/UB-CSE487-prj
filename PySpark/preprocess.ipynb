{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b773345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d665948",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LENGTH = 1024\n",
    "HOP_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b384e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.84.35.181:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>preprocess</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8611e31f50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('preprocess').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc944ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING/PREPROCESSING STEP 1: Take audio from x minutes to 10 seconds\n",
    "def sliceAudio(rdd, duration, sampleRate):\n",
    "  secondsPerSample = 1/sampleRate\n",
    "  samplesForDuration = duration/secondsPerSample\n",
    "  rddZip = rdd.zipWithIndex() # (signal[i], i)\n",
    "  filteredRdd = rddZip.filter(lambda x: x[1] < samplesForDuration)\n",
    "  return filteredRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f836d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 2: Retrieve amplitude envelope\n",
    "def amplitudeEnvelope(rdd, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  envelope = []\n",
    "  for i in range(0, len(signal), hopLength):\n",
    "      frameAE = max(signal[i:i+frameLength])\n",
    "      envelope.append(frameAE)\n",
    "  return spark.sparkContext.parallelize(envelope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "baacff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 3: Retreive Root Mean Square Energy\n",
    "def rms(rdd, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  rms = librosa.feature.rms(y=signal, frame_length=frameLength, hop_length=hopLength)[0]\n",
    "  return spark.sparkContext.parallelize(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d8c0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 4: Retreive Zero Crossing Rate\n",
    "def zcr(rdd, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  zcr = librosa.feature.zero_crossing_rate(signal, frame_length=frameLength, hop_length=hopLength)[0]\n",
    "  return spark.sparkContext.parallelize(zcr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7f99a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 5: Retreive Mel Frequency Cepstrum Coefficients\n",
    "def mfcc20(rdd, sampleRate):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  mfcc = librosa.feature.mfcc(y=signal, n_mfcc=20, sr=sampleRate)\n",
    "  mfccRdds = []\n",
    "  for i in range(len(mfcc)):\n",
    "    mfccRdds.append(spark.sparkContext.parallelize(mfcc[i]))\n",
    "  return mfccRdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bdd7103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 6: Retreive Spectral Centroid\n",
    "def spectralCentroid(rdd, sampleRate, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  sc = librosa.feature.spectral_centroid(y=signal, sr=sampleRate, n_fft=frameLength, hop_length=hopLength)[0]\n",
    "  return spark.sparkContext.parallelize(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47f2e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 7: Retreive Spectral Bandwidth\n",
    "def spectralBandwidth(rdd, sampleRate, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  sb = librosa.feature.spectral_bandwidth(y=signal, sr=sampleRate, n_fft=frameLength, hop_length=hopLength)[0]\n",
    "  return spark.sparkContext.parallelize(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9a8287b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 8: Spectral Rolloff\n",
    "def spectralRolloff(rdd, sampleRate, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  sr = librosa.feature.spectral_rolloff(y=signal, sr=sampleRate, n_fft=frameLength, hop_length=hopLength)[0]\n",
    "  return spark.sparkContext.parallelize(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e43da5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 9: Normalize RDDs\n",
    "def normalize(rdd):\n",
    "  array = np.array(rdd.collect())\n",
    "  # put return val in tuple so we can union and dataframe\n",
    "  return spark.sparkContext.parallelize((array - np.min(array))/(np.max(array)-np.min(array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "413170b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineRdd(amplitudeRdd, rmsRdd, zcrRdd, scRdd, sbRdd, srRdd, mfccRdds):\n",
    "  amplitude = amplitudeRdd.collect()\n",
    "  rms = rmsRdd.collect()\n",
    "  zcr = zcrRdd.collect()\n",
    "  sc = scRdd.collect()\n",
    "  sb = sbRdd.collect()\n",
    "  sr = srRdd.collect()\n",
    "  mfccs = [i.collect() for i in mfccRdds]\n",
    "  rows = []\n",
    "  for i in range(len(rms)):\n",
    "    row = [amplitude[i], rms[i], zcr[i], sc[i], sb[i], sr[i]]\n",
    "    for mfcc in mfccs:\n",
    "      row.append(mfcc[i])\n",
    "    row = [float(i) for i in row]\n",
    "    rows.append(tuple(row))\n",
    "  return rows    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavToDataFrame(input, intelligence):\n",
    "    audioArray, sampleRate = librosa.load(input)\n",
    "    rdd = spark.sparkContext.parallelize(audioArray)\n",
    "    filteredRdd = sliceAudio(rdd, 10, sampleRate)\n",
    "    amplitudeRdd = amplitudeEnvelope(filteredRdd, FRAME_LENGTH, HOP_LENGTH)\n",
    "    rmsRdd = rms(filteredRdd, FRAME_LENGTH, HOP_LENGTH)\n",
    "    zcrRdd = zcr(filteredRdd, FRAME_LENGTH, HOP_LENGTH)\n",
    "    mfccRdds = mfcc20(filteredRdd, sampleRate)\n",
    "    scRdd = spectralCentroid(filteredRdd, sampleRate, FRAME_LENGTH, HOP_LENGTH)\n",
    "    sbRdd = spectralBandwidth(filteredRdd, sampleRate, FRAME_LENGTH, HOP_LENGTH)\n",
    "    srRdd = spectralRolloff(filteredRdd, sampleRate, FRAME_LENGTH, HOP_LENGTH)\n",
    "    amplitudeRdd = normalize(amplitudeRdd)\n",
    "    rmsRdd = normalize(rmsRdd)\n",
    "    zcrRdd = normalize(zcrRdd)\n",
    "    for i in range(len(mfccRdds)):\n",
    "      mfccRdds[i] = normalize(mfccRdds[i])\n",
    "    scRdd = normalize(scRdd)\n",
    "    sbRdd = normalize(sbRdd)\n",
    "    srRdd = normalize(srRdd)\n",
    "\n",
    "    #CLEANING/PREPROCESSING STEP 10: Combine and make to dataframe\n",
    "    rddRows = combineRdd(amplitudeRdd, rmsRdd, zcrRdd, scRdd, sbRdd, srRdd, mfccRdds)\n",
    "    schema = StructType([\n",
    "        StructField(\"amplitudeEnvelope\", FloatType()),\n",
    "        StructField(\"RMSE\", FloatType()),\n",
    "        StructField(\"ZCR\", FloatType()),\n",
    "        StructField(\"spectralCentroid\", FloatType()),\n",
    "        StructField(\"spectralBandwidth\", FloatType()),\n",
    "        StructField(\"spectralRolloff\", FloatType()),\n",
    "        StructField(\"MFCC1\", FloatType()),\n",
    "        StructField(\"MFCC2\", FloatType()),\n",
    "        StructField(\"MFCC3\", FloatType()),\n",
    "        StructField(\"MFCC4\", FloatType()),\n",
    "        StructField(\"MFCC5\", FloatType()),\n",
    "        StructField(\"MFCC6\", FloatType()),\n",
    "        StructField(\"MFCC7\", FloatType()),\n",
    "        StructField(\"MFCC8\", FloatType()),\n",
    "        StructField(\"MFCC9\", FloatType()),\n",
    "        StructField(\"MFCC10\", FloatType()),\n",
    "        StructField(\"MFCC11\", FloatType()),\n",
    "        StructField(\"MFCC12\", FloatType()),\n",
    "        StructField(\"MFCC13\", FloatType()),\n",
    "        StructField(\"MFCC14\", FloatType()),\n",
    "        StructField(\"MFCC15\", FloatType()),\n",
    "        StructField(\"MFCC16\", FloatType()),\n",
    "        StructField(\"MFCC17\", FloatType()),\n",
    "        StructField(\"MFCC18\", FloatType()),\n",
    "        StructField(\"MFCC19\", FloatType()),\n",
    "        StructField(\"MFCC20\", FloatType())\n",
    "    ])\n",
    "\n",
    "    df = spark.createDataFrame(rddRows, schema=schema)\n",
    "    df = df.withColumn(\"intelligence\", list(intelligence))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "919c4765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN] Argument `col` should be a Column, got int.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m bidenDF = \u001b[43mwavToDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../AudioData/biden-human.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m bidenToObamaAiDF = wavToDataFrame(\u001b[33m\"\u001b[39m\u001b[33m../AudioData/biden-to-obama-ai.wav\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m      3\u001b[39m linusDF = wavToDataFrame(\u001b[33m\"\u001b[39m\u001b[33m../AudioData/linus-human.wav\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mwavToDataFrame\u001b[39m\u001b[34m(input, intelligence)\u001b[39m\n\u001b[32m     23\u001b[39m schema = StructType([\n\u001b[32m     24\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mamplitudeEnvelope\u001b[39m\u001b[33m\"\u001b[39m, FloatType()),\n\u001b[32m     25\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mRMSE\u001b[39m\u001b[33m\"\u001b[39m, FloatType()),\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     StructField(\u001b[33m\"\u001b[39m\u001b[33mMFCC20\u001b[39m\u001b[33m\"\u001b[39m, FloatType())\n\u001b[32m     50\u001b[39m ])\n\u001b[32m     52\u001b[39m df = spark.createDataFrame(rddRows, schema=schema)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mintelligence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mintelligence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/UB-Spring-2025/CSE487/UB-CSE487-prj/.venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5172\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   5129\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5130\u001b[39m \u001b[33;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[32m   5131\u001b[39m \u001b[33;03mexisting column that has the same name.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5169\u001b[39m \u001b[33;03m+---+-----+----+\u001b[39;00m\n\u001b[32m   5170\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m-> \u001b[39m\u001b[32m5172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   5173\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5174\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   5175\u001b[39m     )\n\u001b[32m   5176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m._jdf.withColumn(colName, col._jc), \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [NOT_COLUMN] Argument `col` should be a Column, got int."
     ]
    }
   ],
   "source": [
    "bidenDF = wavToDataFrame(\"../AudioData/biden-human.wav\", 0)\n",
    "bidenToObamaAiDF = wavToDataFrame(\"../AudioData/biden-to-obama-ai.wav\", 1)\n",
    "linusDF = wavToDataFrame(\"../AudioData/linus-human.wav\", 0)\n",
    "linusToRyanAiDF = wavToDataFrame(\"../AudioData/linus-to-ryan-ai.wav\", 1)\n",
    "linusToBidenAiDF = wavToDataFrame(\"../AudioData/linus-to-biden-ai.wav\", 1)\n",
    "obamaDF = wavToDataFrame(\"../AudioData/obama-human.wav\", 0)\n",
    "obamaToLinusAiDF= wavToDataFrame(\"AudioData/obama-to-linus-ai.wav\", 1)\n",
    "trumpDF = wavToDataFrame(\"../AudioData/trump-human.wav\", 0)\n",
    "trumpToTaylorAiDF = wavToDataFrame(\"../AudioData/trump-to-taylor-ai.wav\", 1)\n",
    "margotDF = wavToDataFrame(\"../AudioData/margot-human.wav\", 0)\n",
    "margotToTrumpAiDF = wavToDataFrame(\"../AudioData/margot-to-trump-ai.wav\", 1)\n",
    "taylorDF = wavToDataFrame(\"../AudioData/taylor-human.wav\", 0)\n",
    "taylorToMargotAiDF = wavToDataFrame(\"../AudioData/taylor-to-margot-ai.wav\", 1)\n",
    "\n",
    "combinedDf = bidenDF.union(bidenToObamaAiDF).union(linusDF).union(\n",
    "    linusToRyanAiDF).union(linusToBidenAiDF).union(obamaDF).union(\n",
    "    obamaToLinusAiDF).union(trumpDF).union(trumpToTaylorAiDF).union(\n",
    "    margotDF).union(margotToTrumpAiDF).union(taylorDF).union(taylorToMargotAiDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
