{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b773345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d665948",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LENGTH = 1024\n",
    "HOP_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b384e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('preprocess').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc944ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING/PREPROCESSING STEP 1: Take audio from x minutes to 10 seconds\n",
    "def sliceAudio(rdd, duration, sampleRate):\n",
    "  secondsPerSample = 1/sampleRate\n",
    "  samplesForDuration = duration/secondsPerSample\n",
    "  rddZip = rdd.zipWithIndex() # (signal[i], i)\n",
    "  filteredRdd = rddZip.filter(lambda x: x[1] < samplesForDuration)\n",
    "  return filteredRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 2: Retrieve amplitude envelope\n",
    "def amplitudeEnvelope(rdd, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  envelope = []\n",
    "  for i in range(0, len(signal), hopLength):\n",
    "      frameAE = max(signal[i:i+frameLength])\n",
    "      envelope.append(frameAE)\n",
    "  return spark.sparkContext.parallelize(envelope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baacff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 3: Retreive Root Mean Square Energy\n",
    "def rms(rdd, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  rms = librosa.feature.rms(y=signal, frame_length=frameLength, hop_length=hopLength)[0]\n",
    "  return spark.sparkContext.parallelize(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 4: Retreive Zero Crossing Rate\n",
    "def zcr(rdd, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  zcr = librosa.feature.zero_crossing_rate(signal, frame_length=frameLength, hop_length=hopLength)[0]\n",
    "  return spark.sparkContext.parallelize(zcr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 5: Retreive Mel Frequency Cepstrum Coefficients\n",
    "def mfcc20(rdd, sampleRate):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  mfcc = librosa.feature.mfcc(y=signal, n_mfcc=20, sr=sampleRate)\n",
    "  mfccRdds = []\n",
    "  for i in range(len(mfcc)):\n",
    "    mfccRdds.append(spark.sparkContext.parallelize(mfcc[i]))\n",
    "  return mfccRdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 6: Retreive Spectral Centroid\n",
    "def spectralCentroid(rdd, sampleRate, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  sc = librosa.feature.spectral_centroid(y=signal, sr=sampleRate, n_fft=frameLength, hop_length=hopLength)[0]\n",
    "  return spark.sparkContext.parallelize(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 7: Retreive Spectral Bandwidth\n",
    "def spectralBandwidth(rdd, sampleRate, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  sb = librosa.feature.spectral_bandwidth(y=signal, sr=sampleRate, n_fft=frameLength, hop_length=hopLength)[0]\n",
    "  return spark.sparkContext.parallelize(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8287b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 8: Spectral Rolloff\n",
    "def spectralRolloff(rdd, sampleRate, frameLength, hopLength):\n",
    "  rddSort = rdd.sortBy(lambda x: x[1])\n",
    "  signal = np.array([list(i) for i in zip(*rddSort.collect())][0])\n",
    "  sr = librosa.feature.spectral_rolloff(y=signal, sr=sampleRate, n_fft=frameLength, hop_length=hopLength)[0]\n",
    "  return spark.sparkContext.parallelize(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43da5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 9: Normalize RDDs\n",
    "def normalize(rdd):\n",
    "  array = np.array(rdd.collect())\n",
    "  # put return val in tuple so we can union and dataframe\n",
    "  return spark.sparkContext.parallelize((array - np.min(array))/(np.max(array)-np.min(array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413170b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineRdd(amplitudeRdd, rmsRdd, zcrRdd, scRdd, sbRdd, srRdd, mfccRdds):\n",
    "  amplitude = amplitudeRdd.collect()\n",
    "  rms = rmsRdd.collect()\n",
    "  zcr = zcrRdd.collect()\n",
    "  sc = scRdd.collect()\n",
    "  sb = sbRdd.collect()\n",
    "  sr = srRdd.collect()\n",
    "  mfccs = [i.collect() for i in mfccRdds]\n",
    "  rows = []\n",
    "  for i in range(len(rms)):\n",
    "    row = [amplitude[i], rms[i], zcr[i], sc[i], sb[i], sr[i]]\n",
    "    for mfcc in mfccs:\n",
    "      row.append(mfcc[i])\n",
    "    row = [float(i) for i in row]\n",
    "    rows.append(tuple(row))\n",
    "  return rows    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavToDataFrame(input, intelligence):\n",
    "    audioArray, sampleRate = librosa.load(input)\n",
    "    rdd = spark.sparkContext.parallelize(audioArray)\n",
    "    filteredRdd = sliceAudio(rdd, 10, sampleRate)\n",
    "    amplitudeRdd = amplitudeEnvelope(filteredRdd, FRAME_LENGTH, HOP_LENGTH)\n",
    "    rmsRdd = rms(filteredRdd, FRAME_LENGTH, HOP_LENGTH)\n",
    "    zcrRdd = zcr(filteredRdd, FRAME_LENGTH, HOP_LENGTH)\n",
    "    mfccRdds = mfcc20(filteredRdd, sampleRate)\n",
    "    scRdd = spectralCentroid(filteredRdd, sampleRate, FRAME_LENGTH, HOP_LENGTH)\n",
    "    sbRdd = spectralBandwidth(filteredRdd, sampleRate, FRAME_LENGTH, HOP_LENGTH)\n",
    "    srRdd = spectralRolloff(filteredRdd, sampleRate, FRAME_LENGTH, HOP_LENGTH)\n",
    "    amplitudeRdd = normalize(amplitudeRdd)\n",
    "    rmsRdd = normalize(rmsRdd)\n",
    "    zcrRdd = normalize(zcrRdd)\n",
    "    for i in range(len(mfccRdds)):\n",
    "      mfccRdds[i] = normalize(mfccRdds[i])\n",
    "    scRdd = normalize(scRdd)\n",
    "    sbRdd = normalize(sbRdd)\n",
    "    srRdd = normalize(srRdd)\n",
    "\n",
    "    #CLEANING/PREPROCESSING STEP 10: Combine and make to dataframe\n",
    "    rddRows = combineRdd(amplitudeRdd, rmsRdd, zcrRdd, scRdd, sbRdd, srRdd, mfccRdds)\n",
    "    schema = StructType([\n",
    "        StructField(\"amplitudeEnvelope\", FloatType()),\n",
    "        StructField(\"RMSE\", FloatType()),\n",
    "        StructField(\"ZCR\", FloatType()),\n",
    "        StructField(\"spectralCentroid\", FloatType()),\n",
    "        StructField(\"spectralBandwidth\", FloatType()),\n",
    "        StructField(\"spectralRolloff\", FloatType()),\n",
    "        StructField(\"MFCC1\", FloatType()),\n",
    "        StructField(\"MFCC2\", FloatType()),\n",
    "        StructField(\"MFCC3\", FloatType()),\n",
    "        StructField(\"MFCC4\", FloatType()),\n",
    "        StructField(\"MFCC5\", FloatType()),\n",
    "        StructField(\"MFCC6\", FloatType()),\n",
    "        StructField(\"MFCC7\", FloatType()),\n",
    "        StructField(\"MFCC8\", FloatType()),\n",
    "        StructField(\"MFCC9\", FloatType()),\n",
    "        StructField(\"MFCC10\", FloatType()),\n",
    "        StructField(\"MFCC11\", FloatType()),\n",
    "        StructField(\"MFCC12\", FloatType()),\n",
    "        StructField(\"MFCC13\", FloatType()),\n",
    "        StructField(\"MFCC14\", FloatType()),\n",
    "        StructField(\"MFCC15\", FloatType()),\n",
    "        StructField(\"MFCC16\", FloatType()),\n",
    "        StructField(\"MFCC17\", FloatType()),\n",
    "        StructField(\"MFCC18\", FloatType()),\n",
    "        StructField(\"MFCC19\", FloatType()),\n",
    "        StructField(\"MFCC20\", FloatType())\n",
    "    ])\n",
    "\n",
    "    df = spark.createDataFrame(rddRows, schema=schema)\n",
    "    df = df.withColumn(\"intelligence\", lit(intelligence))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidenDF = wavToDataFrame(\"AudioData/biden-human.wav\", 0)\n",
    "bidenToObamaAiDF = wavToDataFrame(\"AudioData/biden-to-obama-ai.wav\", 1)\n",
    "linusDF = wavToDataFrame(\"AudioData/linus-human.wav\", 0)\n",
    "linusToRyanAiDF = wavToDataFrame(\"AudioData/linus-to-ryan-ai.wav\", 1)\n",
    "linusToBidenAiDF = wavToDataFrame(\"AudioData/linus-to-biden-ai.wav\", 1)\n",
    "obamaDF = wavToDataFrame(\"AudioData/obama-human.wav\", 0)\n",
    "obamaToLinusAiDF= wavToDataFrame(\"AudioData/obama-to-linus-ai.wav\", 1)\n",
    "trumpDF = wavToDataFrame(\"AudioData/trump-human.wav\", 0)\n",
    "trumpToTaylorAiDF = wavToDataFrame(\"AudioData/trump-to-taylor-ai.wav\", 1)\n",
    "margotDF = wavToDataFrame(\"AudioData/margot-human.wav\", 0)\n",
    "margotToTrumpAiDF = wavToDataFrame(\"AudioData/margot-to-trump-ai.wav\", 1)\n",
    "taylorDF = wavToDataFrame(\"AudioData/taylor-human.wav\", 0)\n",
    "taylorToMargotAiDF = wavToDataFrame(\"AudioData/taylor-to-margot-ai.wav\", 1)\n",
    "\n",
    "combinedDf = bidenDf.union(bidenToObamaAiDF).union(linusDF).union(\n",
    "    linusToRyanAiDF).union(linusToBidenAiDF).union(obamaDF).union(\n",
    "    obamaToLinusAiDF).union(trumpDF).union(trumpToTaylorAiDF).union(\n",
    "    margotDF).union(margotToTrumpAiDF).union(taylorDF).union(taylorToMargotAiDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
