{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LENGTH = 1024\n",
    "HOP_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 1: Take audio from x minutes to 10 seconds\n",
    "\n",
    "# This step was done outside of the notebook to free space in\n",
    "# the GitHub repository. To see the operation, view the utility.py\n",
    "# file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 2: Remove ambient noise (i.e. crowd cheer, clapping, wind)\n",
    "# There is a caveat to this step in that the magnitude of the ambience cannot be equal\n",
    "# to or greater than the speaker. For this reason, only some files can be cleaned\n",
    "# in this manner.\n",
    "\n",
    "def removeAmbience(inputWav):\n",
    "    audioArray, sampleRate = librosa.load(inputWav)\n",
    "    \n",
    "    # filter out the most minimal signals\n",
    "    magSpec, phase = librosa.magphase(librosa.stft(audioArray)) # D = S*P\n",
    "    specFilter = librosa.decompose.nn_filter(magSpec, aggregate=np.median,\n",
    "                    metric='cosine', width=int(librosa.time_to_frames(2, sr=sampleRate)))\n",
    "    specFilter = np.minimum(magSpec, specFilter)\n",
    "\n",
    "    # apply masks over spectrogram magnitude\n",
    "    ambientMargin = 2\n",
    "    ambientMask = librosa.util.softmask(specFilter,\n",
    "                        (ambientMargin*(magSpec-specFilter)), power=2)\n",
    "    foregroundMargin = 10\n",
    "    foregroundMask = librosa.util.softmask((magSpec-specFilter),\n",
    "                        (foregroundMargin*specFilter), power=2)\n",
    "    ambientSpec = ambientMask*magSpec\n",
    "    foregroundSpec = foregroundMask*magSpec\n",
    "\n",
    "    # reconstruct foreground signal\n",
    "    complexSpec = foregroundSpec*phase\n",
    "\n",
    "    #CLEANING/PREPROCESSING STEP 11: pad lost samples from reconstruction\n",
    "    reconstructSignal = librosa.istft(complexSpec)\n",
    "    padding = len(audioArray)-len(reconstructSignal)\n",
    "    reconstructPadded = np.pad(reconstructSignal, (0, padding), 'constant', constant_values=(0, 0))\n",
    "    \n",
    "    return reconstructPadded, sampleRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 3: Retrieve amplitude envelope\n",
    "\n",
    "def amplitudeEnvelope(signal, frameLength, hopLength):\n",
    "    envelope = []\n",
    "    for i in range(0, len(signal), hopLength):\n",
    "        frameAE = max(signal[i:i+frameLength])\n",
    "        envelope.append(frameAE)\n",
    "    return np.array(envelope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 4: Retreive Root Mean Square Energy\n",
    "\n",
    "def rms(signal, frameLength, hopLength):\n",
    "    return librosa.feature.rms(y=signal, frame_length=frameLength, hop_length=hopLength)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 5: Retreive Zero Crossing Rate\n",
    "\n",
    "def zcr(signal, frameLength, hopLength):\n",
    "    return librosa.feature.zero_crossing_rate(signal, frame_length=frameLength, hop_length=hopLength)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 6: Retreive Mel Frequency Cepstrum Coefficients\n",
    "\n",
    "def mfcc20(signal, sampleRate):\n",
    "    return librosa.feature.mfcc(y=signal, n_mfcc=20, sr=sampleRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 7: Retreive Spectral Centroid\n",
    "\n",
    "def spectralCentroid(signal, sampleRate, frameLength, hopLength):\n",
    "    return librosa.feature.spectral_centroid(y=signal, sr=sampleRate, n_fft=frameLength, hop_length=hopLength)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 8: Retreive Spectral Bandwidth\n",
    "\n",
    "def spectralBandwidth(signal, sampleRate, frameLength, hopLength):\n",
    "    return librosa.feature.spectral_bandwidth(y=signal, sr=sampleRate, n_fft=frameLength, hop_length=hopLength)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING/PREPROCESSING STEP 9: Spectral Rolloff\n",
    "def spectralRolloff(signal, sampleRate, frameLength, hopLength):\n",
    "    return librosa.feature.spectral_rolloff(y=signal, sr=sampleRate, n_fft=frameLength, hop_length=hopLength)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array):\n",
    "    return (array - np.min(array))/(np.max(array)-np.min(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be final function\n",
    "def wavToDataFrame(inputWav, filename, intelligence, filterAmbience=False):\n",
    "    if filterAmbience:\n",
    "        audioArray, sampleRate = removeAmbience(inputWav)\n",
    "    else:\n",
    "        audioArray, sampleRate = librosa.load(inputWav)\n",
    "\n",
    "    #CLEANING/PREPROCESSING STEP 9: Include Binary Classifier\n",
    "\n",
    "    frames = range(0, math.ceil(len(audioArray)/512))\n",
    "    fn = [filename for i in range(len(frames))]\n",
    "    i = [intelligence for i in range(len(frames))]\n",
    "    data = {\"frameID\": frames, \"filename\": fn, \"intelligence\": i}\n",
    "    df = pd.DataFrame(data=data)\n",
    "\n",
    "    AE = amplitudeEnvelope(audioArray, FRAME_LENGTH, HOP_LENGTH)\n",
    "    RMSE = rms(audioArray, FRAME_LENGTH, HOP_LENGTH)\n",
    "    ZCR = zcr(audioArray, FRAME_LENGTH, HOP_LENGTH)    \n",
    "    SC = spectralCentroid(audioArray, sampleRate, FRAME_LENGTH, HOP_LENGTH)\n",
    "    SB = spectralBandwidth(audioArray, sampleRate, FRAME_LENGTH, HOP_LENGTH)\n",
    "    SR = spectralRolloff(audioArray, sampleRate, FRAME_LENGTH, HOP_LENGTH)\n",
    "\n",
    "    #CLEANING/PREPROCESSING STEP 10: Normalize Arrays\n",
    "    \n",
    "    df[\"amplitudeEnvelope\"] = normalize(AE)\n",
    "    df[\"RMSE\"] = normalize(RMSE)\n",
    "    df[\"ZCR\"] = normalize(ZCR)\n",
    "\n",
    "    mfcc = mfcc20(audioArray, sampleRate)\n",
    "    for i in range(len(mfcc)):\n",
    "        feature = \"MFCC\"+str(i+1)\n",
    "        df[feature] = normalize(mfcc[i])\n",
    "    \n",
    "    df[\"spectralCentroid\"] = normalize(SC)\n",
    "    df[\"spectralBandwidth\"] = normalize(SB)\n",
    "    df[\"spectralRolloff\"] = normalize(SR)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidenDF = wavToDataFrame(\"AudioData/biden-human.wav\", \"biden-human.wav\", \"Human\", filterAmbience=False)\n",
    "bidenAiDF = wavToDataFrame(\"AudioData/biden-to-obama-ai.wav\", \"biden-to-obama-ai.wav\", \"AI\", filterAmbience=False)\n",
    "linusDF = wavToDataFrame(\"AudioData/linus-human.wav\", \"linus-human.wav\", \"Human\", filterAmbience=False)\n",
    "linusAiDF = wavToDataFrame(\"AudioData/linus-to-ryan-ai.wav\", \"linus-to-ryan-ai.wav\", \"AI\", filterAmbience=False)\n",
    "margotDF = wavToDataFrame(\"AudioData/margot-human.wav\", \"margot-human.wav\", \"Human\", filterAmbience=True)\n",
    "margotAiDF = wavToDataFrame(\"AudioData/margot-to-trump-ai.wav\", \"margot-to-trump-ai.wav\", \"AI\", filterAmbience=True)\n",
    "taylorDF = wavToDataFrame(\"AudioData/taylor-human.wav\", \"taylor-human.wav\", \"Human\", filterAmbience=True)\n",
    "taylorAiDF = wavToDataFrame(\"AudioData/taylor-to-margot-ai.wav\", \"taylor-to-margot-ai.wav\", \"AI\", filterAmbience=True)\n",
    "\n",
    "# Concat dataframes\n",
    "df = pd.concat([bidenDF, bidenAiDF, linusDF, linusAiDF, margotDF, margotAiDF, taylorDF, taylorAiDF])\n",
    "\n",
    "# create CSV\n",
    "df.to_csv(\"./wavData.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
